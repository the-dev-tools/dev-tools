name: Performance CI

on:
  workflow_dispatch:
    inputs:
      benchmark_count:
        description: 'Number of benchmark runs'
        required: false
        default: '3'
        type: string
      benchmark_timeout:
        description: 'Benchmark timeout in minutes'
        required: false
        default: '30'
        type: string
      force_comparison:
        description: 'Force comparison even if no previous artifacts'
        required: false
        default: false
        type: boolean
  pull_request:
    paths:
      - 'packages/server/pkg/flow/simulation/**'
  push:
    paths:
      - 'packages/server/pkg/flow/simulation/**'
    branches: [main]

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  benchmark:
    name: Run Benchmarks
    runs-on: ubuntu-latest
    outputs:
      benchmark-results: ${{ steps.parse-results.outputs.results }}
      has-changes: ${{ steps.check-changes.outputs.changed }}
    steps:
      - uses: actions/checkout@v4

      - name: Setup runner environment
        uses: ./.github/actions/setup

      - name: Check if simulation package changed
        id: check-changes
        run: |
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            echo "changed=true" >> $GITHUB_OUTPUT
          else
            # Get changed files
            if [ "${{ github.event_name }}" == "pull_request" ]; then
              BASE="${{ github.event.pull_request.base.sha }}"
            else
              BASE="${{ github.event.before }}"
            fi
            
            # Check if any files in simulation package changed
            CHANGED=$(git diff --name-only $BASE...HEAD | grep -E "^packages/server/pkg/flow/simulation/" || true)
            if [ -n "$CHANGED" ]; then
              echo "changed=true" >> $GITHUB_OUTPUT
            else
              echo "changed=false" >> $GITHUB_OUTPUT
            fi
          fi

      - name: Run Go benchmarks
        id: run-benchmarks
        if: steps.check-changes.outputs.changed == 'true' || github.event_name == 'workflow_dispatch'
        run: |
          cd packages/server/pkg/flow/simulation

          # Create benchmark results directory
          mkdir -p benchmark-results

          # Get input parameters with defaults
          BENCHMARK_COUNT="${{ github.event.inputs.benchmark_count || '3' }}"
          BENCHMARK_TIMEOUT="${{ github.event.inputs.benchmark_timeout || '30' }}"

          echo "Running benchmarks with count=$BENCHMARK_COUNT, timeout=${BENCHMARK_TIMEOUT}m"

          # Run benchmarks with memory profiling
          nix develop -c go test -bench=. -benchmem -run=^$ -count=$BENCHMARK_COUNT -timeout=${BENCHMARK_TIMEOUT}m \
            -json ./... > benchmark-results/bench.json 2>&1 || true

          # Also generate traditional format for parsing
          nix develop -c go test -bench=. -benchmem -run=^$ -count=$BENCHMARK_COUNT -timeout=${BENCHMARK_TIMEOUT}m \
            ./... > benchmark-results/bench.txt 2>&1 || true

          echo "Benchmark results saved to benchmark-results/"

      - name: Parse benchmark results
        id: parse-results
        if: steps.check-changes.outputs.changed == 'true' || github.event_name == 'workflow_dispatch'
        run: |
          cd packages/server/pkg/flow/simulation

          # Run benchmark parser directly via go run
          go run ../../../../../.github/scripts/parse-benchmarks.go benchmark-results/bench.txt

          # Set output for GitHub Actions (basic results for now)
          if [ -f "benchmark-results/results.json" ]; then
            # Create a simple markdown summary
            echo "## üöÄ Flow Simulation Performance Results" > benchmark-results/results.md
            echo "" >> benchmark-results/results.md
            echo "*Benchmark results parsed and ready for comparison*" >> benchmark-results/results.md
            echo "" >> benchmark-results/results.md
            echo "View detailed comparison in the PR comment below." >> benchmark-results/results.md
            
            # Escape the markdown for GitHub Actions output
            RESULTS=$(cat benchmark-results/results.md | sed 's/`/\\`/g' | sed 's/$/\\n/' | tr -d '\n')
            echo "results<<EOF" >> $GITHUB_OUTPUT
            echo "$RESULTS" >> $GITHUB_OUTPUT
            echo "EOF" >> $GITHUB_OUTPUT
          fi

      - name: Download previous benchmark artifacts
        if: (steps.check-changes.outputs.changed == 'true' || github.event_name == 'workflow_dispatch') && (github.event.inputs.force_comparison == 'true' || github.event_name != 'workflow_dispatch')
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          pattern: benchmark-results-*
          path: previous-artifacts/
          merge-multiple: true

      - name: Find and prepare previous results
        if: (steps.check-changes.outputs.changed == 'true' || github.event_name == 'workflow_dispatch') && (github.event.inputs.force_comparison == 'true' || github.event_name != 'workflow_dispatch')
        run: |
          cd packages/server/pkg/flow/simulation

          # Find the most recent successful run artifacts
          if [ -d "../../previous-artifacts" ]; then
            # Look for the most recent results.json file
            PREV_RESULT=$(find ../../previous-artifacts -name "results.json" -type f | head -1)
            if [ -n "$PREV_RESULT" ]; then
              echo "Found previous results: $PREV_RESULT"
              mkdir -p previous
              cp "$PREV_RESULT" previous/results.json
              echo "‚úÖ Previous benchmark results loaded for comparison"
            else
              echo "üÜï No previous benchmark results found - first run"
            fi
          else
            echo "üÜï No previous artifacts found - first run"
          fi

      - name: Compare benchmark results
        id: compare-results
        if: steps.check-changes.outputs.changed == 'true' || github.event_name == 'workflow_dispatch'
        run: |
          cd packages/server/pkg/flow/simulation

          # Run comparison tool directly via go run
          go run ../../../../../.github/scripts/compare-benchmarks.go || {
            echo "Comparison completed with regressions detected"
            echo "has-regressions=true" >> $GITHUB_OUTPUT
          }

          # Set default if no regressions
          if [ ! -f "$GITHUB_OUTPUT" ] || ! grep -q "has-regressions=true" "$GITHUB_OUTPUT"; then
            echo "has-regressions=false" >> $GITHUB_OUTPUT
          fi

          # Set comparison output for GitHub Actions
          if [ -f "benchmark-results/comparison.md" ]; then
            # Escape the markdown for GitHub Actions output
            COMPARISON=$(cat benchmark-results/comparison.md | sed 's/`/\\`/g' | sed 's/$/\\n/' | tr -d '\n')
            echo "comparison<<EOF" >> $GITHUB_OUTPUT
            echo "$COMPARISON" >> $GITHUB_OUTPUT
            echo "EOF" >> $GITHUB_OUTPUT
          fi

      - name: Upload benchmark artifacts
        if: steps.check-changes.outputs.changed == 'true' || github.event_name == 'workflow_dispatch'
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ github.run_number }}
          path: packages/server/pkg/flow/simulation/benchmark-results/
          retention-days: 30

  comment:
    name: Post Results Comment
    runs-on: ubuntu-latest
    needs: benchmark
    if: |
      always() && 
      needs.benchmark.result == 'success' && 
      needs.benchmark.outputs.has-changes == 'true' &&
      github.event_name == 'pull_request'
    steps:
      - name: Find and update PR comment
        uses: actions/github-script@v7
        with:
          script: |
            const comparison = `${{ needs.benchmark.outputs.comparison }}`;
            const hasRegressions = `${{ needs.benchmark.outputs.has-regressions }}` === 'true';

            // Find existing performance comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const existingComment = comments.find(comment => 
              comment.user.type === 'Bot' && 
              (comment.body.includes('üöÄ Flow Simulation Performance Results') || 
               comment.body.includes('üìä Performance Comparison'))
            );

            // Build comment body with comparison data
            let commentBody = comparison;

            // Add regression warning if needed
            if (hasRegressions) {
              commentBody = '\n‚ö†Ô∏è **Performance regressions detected! See details below.**\n\n' + commentBody;
            }

            if (existingComment) {
              // Update existing comment
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: existingComment.id,
                body: commentBody,
              });
              console.log('Updated existing performance comment');
            } else {
              // Create new comment
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: commentBody,
              });
              console.log('Created new performance comment');
            }

  summary:
    name: Performance Summary
    runs-on: ubuntu-latest
    needs: benchmark
    if: always() && (needs.benchmark.outputs.has-changes == 'true' || github.event_name == 'workflow_dispatch')
    steps:
      - name: Download benchmark results
        if: needs.benchmark.result == 'success'
        uses: actions/download-artifact@v4
        with:
          name: benchmark-results-${{ github.run_number }}
          path: benchmark-results/
        continue-on-error: true

      - name: Download benchmark results (workflow_dispatch fallback)
        if: needs.benchmark.result == 'success' && github.event_name == 'workflow_dispatch'
        uses: actions/download-artifact@v4
        with:
          pattern: benchmark-results-*
          path: benchmark-results/
          merge-multiple: true
        continue-on-error: true

      - name: Display performance summary
        if: needs.benchmark.result == 'success'
        run: |
          if [ -f "benchmark-results/comparison.md" ]; then
            echo "## Performance Summary"
            echo ""
            cat benchmark-results/comparison.md
          elif [ -f "benchmark-results/results.md" ]; then
            echo "## Performance Summary"
            echo ""
            cat benchmark-results/results.md
          else
            echo "No benchmark results available"
          fi

      - name: Performance check status
        if: needs.benchmark.result == 'success'
        run: |
          # Check for performance regressions
          if [ -f "benchmark-results/comparison.json" ]; then
            echo "‚úÖ Benchmark comparison completed successfully"
            
            # Check if regressions were detected
            if [ "${{ needs.benchmark.outputs.has-regressions }}" == "true" ]; then
              echo "üö® Performance regressions detected!"
              echo "See the comparison details above for specific benchmarks that need attention."
              exit 1  # Fail the job for CI visibility
            else
              echo "‚úÖ No performance regressions detected"
            fi
          elif [ -f "benchmark-results/results.json" ]; then
            echo "‚úÖ Benchmarks completed successfully (no comparison data available)"
          else
            echo "‚ùå Benchmark results not found"
            exit 1
          fi
